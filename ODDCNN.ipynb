{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dynamic Convolutional Neural Network\n",
    "***\n",
    "# Table of Contents\n",
    "1.   [Imports](#Imports)\n",
    "2.   [Dataset Object](#Dataset-Object)\n",
    "3.   [Data Loading](#Data-Loading)\n",
    "4.   [Model](#Model)\n",
    "5.   [Setup](#Setup)\n",
    "6.   [Training](#Training)\n",
    "7.   [Testing](#Testing)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "\n",
    "The necessary libraries are imported at this stage.\n",
    "\n",
    "* torch - Python pytorch library used to create and train the CNN\n",
    "* numpy - Efficient data arrays\n",
    "* pandas - Data applications\n",
    "* sklearn - Provides a number of models, metrics and general functionality for machine learning.\n",
    "* matplotlib - Provides plotting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import Linear, Conv1d, MaxPool1d, Module, CrossEntropyLoss, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, fbeta_score, precision_score, recall_score, confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Object\n",
    "\n",
    "Although not fully required it is helpful to create a custom torch.utils.data.Dataset when using the pytorch library.\n",
    "This makes the splitting and feeding of data into the network easier.\n",
    "\n",
    "## Encoding\n",
    "\n",
    "A dict is defined 0-2 for the Dynamic activities in the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activity_encode =   {\n",
    "                        \"WALKING\": 0,\n",
    "                        \"STAIRS DN\": 1,\n",
    "                        \"STAIRS UP\": 2,\n",
    "                        \"SWIMMING\" : 3,\n",
    "                        \"PUSH UP\" : 4,\n",
    "                        \"JUMPING\" : 5,\n",
    "                        \"DRIVING\": 6,\n",
    "                        \"LAYING\": 7,\n",
    "                        \"SITTING\": 8,\n",
    "                        \"STANDING\": 9\n",
    "                    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "The object is defined as below for the UCI dataset. 3 functions need to be implemented **__init__**, **__len__** and\n",
    "**__getitem__**.\n",
    "\n",
    "In initialisation the data is split into X and y variables and turned into tensors.\n",
    "\n",
    "The X variable has shape (row_count, 1, feature_count). 1 since there is only one channel of data.\n",
    "\n",
    "The static features are removed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UCI_Dynamic_Dataset(Dataset):\n",
    "    \"\"\"UCI dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, train=False):\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "        data['Label'] = data['Label'].map(activity_encode)\n",
    "        data, _ = [x for _, x in data.groupby(data['Label'] > 5)]\n",
    "        if train:\n",
    "            # shuffle data\n",
    "            data = shuffle(data)\n",
    "            # Get label value counts\n",
    "            temp = data[\"Label\"].value_counts()\n",
    "            # Get the smallest value/ smallest feature count\n",
    "            smallestV = int(temp.values[-1])\n",
    "            # define counts\n",
    "            label_counts = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[]}\n",
    "            # add each index to its label\n",
    "            labels = data[\"Label\"]\n",
    "            for i, label in enumerate(labels):\n",
    "                label_counts[label].append(i)\n",
    "            # re define data dataframe by slicing indices\n",
    "            new_df = pd.DataFrame(columns=data.columns)\n",
    "            for label in label_counts:\n",
    "                new_df = new_df.append(data.iloc[label_counts[label][:smallestV]])\n",
    "\n",
    "            # return data as with even distribution\n",
    "            data = new_df.copy()\n",
    "\n",
    "        self.data_y = data['Label'].values\n",
    "        data = pd.DataFrame(data.drop(['Label'],axis=1))\n",
    "        self.data_x = np.array(data)\n",
    "        # [batch, channels, features]\n",
    "        self.data_x = self.data_x.reshape(len(self.data_x), 1, 589)\n",
    "        self.data_x  = torch.from_numpy(self.data_x)\n",
    "        self.data_y = self.data_y.astype(int)\n",
    "        self.data_y = torch.from_numpy(self.data_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx], self.data_y[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialise the 3 datasets that will be used.\n",
    "\n",
    "# 80 train 20 valid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = UCI_Dynamic_Dataset(csv_file='Our Dataset/train.csv', train=True)\n",
    "valid_data = UCI_Dynamic_Dataset(csv_file='Our Dataset/valid.csv', train=True)\n",
    "test_data = UCI_Dynamic_Dataset(csv_file='Our Dataset/test.csv')\n",
    "\n",
    "def pie(csv_file, train=False):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data['Label'] = data['Label'].map(activity_encode)\n",
    "    data, _ = [x for _, x in data.groupby(data['Label'] > 5)]\n",
    "    if train:\n",
    "        # shuffle data\n",
    "        data = shuffle(data)\n",
    "        # Get label value counts\n",
    "        temp = data[\"Label\"].value_counts()\n",
    "        # Get the smallest value/ smallest feature count\n",
    "        smallestV = int(temp.values[-1])\n",
    "        # define counts\n",
    "        label_counts = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[]}\n",
    "        # add each index to its label\n",
    "        labels = data[\"Label\"]\n",
    "        for i, label in enumerate(labels):\n",
    "            label_counts[label].append(i)\n",
    "        # re define data dataframe by slicing indices\n",
    "        new_df = pd.DataFrame(columns=data.columns)\n",
    "        for label in label_counts:\n",
    "            new_df = new_df.append(data.iloc[label_counts[label][:smallestV]])\n",
    "\n",
    "        # return data as with even distribution\n",
    "        data = new_df.copy()\n",
    "    temp = data[\"Label\"].value_counts()\n",
    "    df = pd.DataFrame({'labels': temp.index,\n",
    "                       'values': temp.values\n",
    "                      })\n",
    "\n",
    "    labels = df['labels']\n",
    "    sizes = df['values']\n",
    "    colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral', 'cyan','lightpink']\n",
    "    patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=90, pctdistance=1.1, labeldistance=1.2)\n",
    "    plt.legend(patches, labels, loc=\"best\")\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "pie('Our Dataset/train.csv', train=True)\n",
    "pie('Our Dataset/valid.csv', train=True)\n",
    "pie('Our Dataset/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading\n",
    "\n",
    "Using the DataLoader object load in the data, shuffling and assigning a batch size of 64."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "The Dynamic CNN is implemented below, documentation is provided in the report."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DynamicCNN(Module):\n",
    "    def __init__(self):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv1d(1, 100, kernel_size=tuple([3]))\n",
    "        self.pool = MaxPool1d(kernel_size=tuple([3]))\n",
    "        self.fc = Linear(19500, 6)\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "    # Defining the forward pass\n",
    "    def forward(self, x):\n",
    "        # print(x.size()) #([64, 1, 589])\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # print(x.size()) #([64, 100, 195])\n",
    "        x = x.view(-1, 19500)\n",
    "        # print(x.size()) #([64, 19500])\n",
    "        x = self.dropout(F.log_softmax((self.fc(x))))\n",
    "        # print(x.size()) #([64, 4])\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "In this step we initialise the loss, optimizer and model objects. For this problem we used cross entropy loss and the\n",
    "adam optimizer.\n",
    "\n",
    "If found we use cuda GPU acceleration to make the process faster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = DynamicCNN().double()\n",
    "epochs = 11\n",
    "\n",
    "step = 1\n",
    "optimizer = Adam(model.parameters(), lr=0.00005)\n",
    "# epochs = 5\n",
    "#\n",
    "# step = epochs\n",
    "# optimizer = Adam(model.parameters(), lr=0.00005)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "In this step the model is trained. Validation is done so we can observe the progress."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loaders = {}\n",
    "data_loaders['train'] = trainloader\n",
    "data_loaders['val'] = validloader\n",
    "data_lengths = {\"train\": len(train_data), \"val\": len(valid_data)}\n",
    "for epoch in range(epochs):\n",
    "    if (epoch+1) % step == 0:\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train(True)  # Set model to training mode\n",
    "        else:\n",
    "            model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device).double(), labels.to(device).long()\n",
    "            # forward pass to get outputs\n",
    "            output = model(inputs)\n",
    "            # if labels == torch.tensor(1):\n",
    "            #     print(labels)\n",
    "            #     print(output)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                # update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / data_lengths[phase]\n",
    "        if (epoch+1) % step == 0:\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n",
    "\n",
    "Finally the model is tested on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.cpu()\n",
    "predictions = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.double(), labels.long()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for p in predicted:\n",
    "            # print(labels)\n",
    "            # print(p)\n",
    "            predictions.append(p)\n",
    "\n",
    "        for l in labels:\n",
    "            test_labels.append(l)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Confusion Matrix maps out the predicted label given to the data with the actual label\n",
    "#Helps us check the rate of true/false positives and true/false negatives\n",
    "#parameters are the true labels, and the predicted labels\n",
    "\n",
    "# https://sklearn.org/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(test_labels, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "classes =    {\n",
    "                        \"WALKING\": 0,\n",
    "                        \"STAIRS DN\": 1,\n",
    "                        \"STAIRS UP\": 2,\n",
    "                        \"SWIMMING\" : 3,\n",
    "                        \"PUSH UP\" : 4,\n",
    "                        \"JUMPING\" : 5,\n",
    "                    }\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,title='Confusion matrix')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(test_labels, predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(test_labels, predictions, beta = 0.5,average='weighted')))\n",
    "print(\"Final precision score on the testing data: {:.4f}\".format(precision_score(test_labels, predictions, average='weighted')))\n",
    "print(\"Final recall score on the testing data: {:.4f}\".format(recall_score(test_labels, predictions, average='weighted')))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}