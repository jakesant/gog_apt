{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dynamic Convolutional Neural Network\n",
    "***\n",
    "# Table of Contents\n",
    "1.   [Imports](#Imports)\n",
    "2.   [Dataset Object](#Dataset-Object)\n",
    "3.   [Data Loading](#Data-Loading)\n",
    "4.   [Model](#Model)\n",
    "5.   [Setup](#Setup)\n",
    "6.   [Training](#Training)\n",
    "7.   [Testing](#Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "\n",
    "The necessary libraries are imported at this stage.\n",
    "\n",
    "* torch - Python pytorch library used to create and train the CNN\n",
    "* numpy - Efficient data arrays\n",
    "* pandas - Data applications"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import Linear, Conv1d, MaxPool1d, Module, CrossEntropyLoss, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Object\n",
    "\n",
    "Although not fully required it is helpful to create a custom torch.utils.data.Dataset when using the pytorch library.\n",
    "This makes the splitting and feeding of data into the network easier.\n",
    "\n",
    "## Encoding\n",
    "\n",
    "A dict is defined 0-2 for the Dynamic activities in the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "activity_encode = {\"WALKING\":0,\n",
    "       \"WALKING_DOWNSTAIRS\":1,\n",
    "       \"WALKING_UPSTAIRS\":2,\n",
    "       \"LAYING\":3,\n",
    "       \"SITTING\":4,\n",
    "       \"STANDING\":5\n",
    "       }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "The object is defined as below for the UCI dataset. 3 functions need to be implemented **__init__**, **__len__** and\n",
    "**__getitem__**.\n",
    "\n",
    "In initialisation the data is split into X and y variables and turned into tensors.\n",
    "\n",
    "The X variable has shape (row_count, 1, feature_count). 1 since there is only one channel of data.\n",
    "\n",
    "The static features are removed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class UCI_Dynamic_Dataset(Dataset):\n",
    "    \"\"\"UCI dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        data['Activity'] = data['Activity'].map(activity_encode)\n",
    "        data, _ = [x for _, x in data.groupby(data['Activity'] > 2)]\n",
    "        self.data_y = data['Activity'].values\n",
    "        data = pd.DataFrame(data.drop(['Activity','subject'],axis=1))\n",
    "        self.data_x = np.array(data)\n",
    "        # [batch, channels, features]\n",
    "        self.data_x = self.data_x.reshape(len(self.data_x), 1, 561)\n",
    "        self.data_x  = torch.from_numpy(self.data_x)\n",
    "        self.data_y = self.data_y.astype(int)\n",
    "        self.data_y = torch.from_numpy(self.data_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx], self.data_y[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialise the 3 datasets that will be used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_data = UCI_Dynamic_Dataset(csv_file='train.csv')\n",
    "valid_data = UCI_Dynamic_Dataset(csv_file='valid.csv')\n",
    "test_data = UCI_Dynamic_Dataset(csv_file='test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading\n",
    "\n",
    "Using the DataLoader object load in the data, shuffling and assigning a batch size of 64."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "The Dynamic CNN is implemented below, documentation is provided in the report."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DynamicCNN(Module):\n",
    "    def __init__(self):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv1d(1, 100, kernel_size=tuple([3]))\n",
    "        self.pool =  MaxPool1d(kernel_size=tuple([3]))\n",
    "        self.fc = Linear(18600, 3)\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "    # Defining the forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 1 * 100 * 186)\n",
    "        x = self.dropout(F.softmax(self.fc(x), dim=1))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "In this step we intiialise the loss, optimizer and model objects. For this problem we used cross entropy loss and the\n",
    "adam optimizer.\n",
    "\n",
    "If found we use cuda GPU acceleration to make the process faster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "DynamicCNN(\n  (conv1): Conv1d(1, 100, kernel_size=(3,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=(3,), stride=(3,), padding=0, dilation=1, ceil_mode=False)\n  (fc): Linear(in_features=18600, out_features=3, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = DynamicCNN()\n",
    "epochs = 10\n",
    "\n",
    "step = epochs//5\n",
    "optimizer = Adam(model.parameters(), lr=0.0005)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "In this step the model is trained. Validation is done so we can observe the progress."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.0134\n",
      "val Loss: 0.0114\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.0114\n",
      "val Loss: 0.0108\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.0111\n",
      "val Loss: 0.0100\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.0110\n",
      "val Loss: 0.0103\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.0110\n",
      "val Loss: 0.0097\n"
     ]
    }
   ],
   "source": [
    "data_loaders = {}\n",
    "data_loaders['train'] = trainloader\n",
    "data_loaders['val'] = validloader\n",
    "data_lengths = {\"train\": len(train_data), \"val\": len(valid_data)}\n",
    "for epoch in range(epochs):\n",
    "    if (epoch+1) % step == 0:\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train(True)  # Set model to training mode\n",
    "        else:\n",
    "            model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for data in data_loaders[phase]:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).long()\n",
    "            # forward pass to get outputs\n",
    "            output = model(inputs)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                # update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / data_lengths[phase]\n",
    "        if (epoch+1) % step == 0:\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n",
    "\n",
    "Finally the model is tested on the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 95 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.cpu()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.float(), labels.long()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test set: %d %%' % (\n",
    "    100 * correct / total))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}